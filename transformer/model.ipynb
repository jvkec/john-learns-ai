{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ccf82af",
   "metadata": {},
   "source": [
    "# Attention is All You Need: My Guide to Transformers\n",
    "\n",
    "*Understanding the architecture that powers modern AI*\n",
    "\n",
    "Welcome to the world of Transformers! The Transformer is a passive component that transfers electrical energy from one circuit to another (or many). \n",
    "\n",
    "Just kidding (ECE joke.. Sorry!)\n",
    "\n",
    "Transformers are the neural networks that power ChatGPT, Google Translate, code completion tools, and essentially all modern AI systems. This guide will walk you through building a Transformer from scratch (not really from scratch.. We are using PyTorch), explaining both the theory and implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba20747",
   "metadata": {},
   "source": [
    "\n",
    "## What is a Transformer?\n",
    "\n",
    "A Transformer is a neural network architecture that uses attention mechanisms to understand relationships between words in a sequence. Unlike traditional recurrent networks (RNNs, LSTMs), Transformers can process entire sequences simultaneously, making them both faster to train and better at capturing long-range dependencies.\n",
    "\n",
    "**Key Applications:**\n",
    "- ChatGPT and other large language models\n",
    "- Google Translate and machine translation\n",
    "- Code completion tools like GitHub Copilot\n",
    "- Image generation models like DALL-E\n",
    "- Scientific research applications\n",
    "\n",
    "**Why Transformers Matter:** Understanding this architecture is essential for anyone working in AI, as it forms the foundation of virtually all modern language models and many other AI applications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a186468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All packages already installed!\n",
      "Environment setup complete! Ready to build a Transformer from scratch.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages if not already installed\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.functional as F\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    print(\"All packages already installed!\")\n",
    "except ImportError as e:\n",
    "    print(f\"Installing missing packages: {e}\")\n",
    "    import subprocess\n",
    "    import sys\n",
    "    \n",
    "    packages = ['torch', 'torchvision', 'matplotlib', 'numpy']\n",
    "    for package in packages:\n",
    "        try:\n",
    "            subprocess.check_call([sys.executable, '-m', 'pip', 'install', package])\n",
    "            print(f\"Successfully installed {package}\")\n",
    "        except subprocess.CalledProcessError:\n",
    "            print(f\"Failed to install {package}\")\n",
    "    \n",
    "    # Try importing again\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.functional as F\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "\n",
    "# Import required libraries\n",
    "import math\n",
    "from typing import Optional\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Environment setup complete! Ready to build a Transformer from scratch.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0396caed",
   "metadata": {},
   "source": [
    "## Understanding Attention Mechanisms\n",
    "\n",
    "Before implementing the code, let's understand the core concept: **Attention**.\n",
    "\n",
    "### What is Attention?\n",
    "\n",
    "Attention is a mechanism that allows the model to focus on different parts of the input sequence when processing each element. When you read \"The cat that I saw yesterday was sleeping on the mat,\" your brain automatically connects \"was sleeping\" to \"cat\" rather than \"mat\" or \"yesterday.\" This is essentially what attention mechanisms do computationally.\n",
    "\n",
    "### The Attention Formula\n",
    "\n",
    "The mathematical foundation of attention is:\n",
    "\n",
    "```\n",
    "Attention(Q, K, V) = softmax(QK^T / √d_k)V\n",
    "```\n",
    "\n",
    "Where:\n",
    "- **Q (Query)**: \"What am I looking for?\"\n",
    "- **K (Key)**: \"What do I represent?\"\n",
    "- **V (Value)**: \"What information do I contain?\"\n",
    "\n",
    "We'll implement this step by step to understand how it works.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self-Attention Implementation\n",
    "# The core mechanism that allows words to attend to each other\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Self-Attention mechanism - the foundation of Transformers\n",
    "    \n",
    "    This allows each word in a sequence to attend to every other word,\n",
    "    determining which relationships are most important for understanding\n",
    "    the context and meaning.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, d_k: int = None):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_k or d_model  # Key dimension (usually same as model dimension)\n",
    "        \n",
    "        # Linear projections for Query, Key, and Value matrices\n",
    "        self.W_q = nn.Linear(d_model, d_k)  # Query projection\n",
    "        self.W_k = nn.Linear(d_model, d_k)  # Key projection  \n",
    "        self.W_v = nn.Linear(d_model, d_model)  # Value projection\n",
    "        \n",
    "        # Scaling factor to prevent softmax from becoming too peaked\n",
    "        self.scale = math.sqrt(d_k)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of self-attention\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, seq_len, d_model)\n",
    "            \n",
    "        Returns:\n",
    "            Output tensor of same shape as input\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, d_model = x.shape\n",
    "        \n",
    "        # Step 1: Create Q, K, V matrices\n",
    "        Q = self.W_q(x)  # (batch_size, seq_len, d_k)\n",
    "        K = self.W_k(x)  # (batch_size, seq_len, d_k) \n",
    "        V = self.W_v(x)  # (batch_size, seq_len, d_model)\n",
    "        \n",
    "        # Step 2: Calculate attention scores\n",
    "        # This determines how much each word should attend to every other word\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale\n",
    "        # Shape: (batch_size, seq_len, seq_len)\n",
    "        \n",
    "        # Step 3: Apply softmax to get attention weights (probabilities)\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        \n",
    "        # Step 4: Apply attention weights to values\n",
    "        # Each word gets a weighted combination of all other words' information\n",
    "        output = torch.matmul(attention_weights, V)\n",
    "        # Shape: (batch_size, seq_len, d_model)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "# Test the self-attention mechanism\n",
    "print(\"Testing Self-Attention...\")\n",
    "\n",
    "# Create a simple example\n",
    "batch_size = 1\n",
    "seq_len = 4  # 4 words\n",
    "d_model = 8  # Each word represented by 8 numbers\n",
    "\n",
    "# Create random input (pretend these are word embeddings)\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "\n",
    "# Create and test self-attention layer\n",
    "self_attn = SelfAttention(d_model)\n",
    "output, attention_weights = self_attn(x)\n",
    "\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {attention_weights.shape}\")\n",
    "print(f\"Attention weights sum to 1: {attention_weights.sum(dim=-1)}\")  # Should be close to 1\n",
    "\n",
    "print(\"Self-Attention is working correctly!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b410ef5",
   "metadata": {},
   "source": [
    "## Multi-Head Attention\n",
    "\n",
    "While single-head attention is powerful, Multi-Head Attention provides multiple perspectives on the same input. Each attention head can focus on different types of relationships - one might focus on syntax, another on semantics, another on long-range dependencies.\n",
    "\n",
    "**Why Multiple Heads?** Different heads can specialize in different aspects of the input, similar to having a team of experts rather than a single person trying to understand everything.\n",
    "\n",
    "**The Original Design:** The paper used 8 heads, which has become a common choice, though the optimal number depends on the specific task and model size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Multi-Head Attention...\n",
      "Input shape: torch.Size([1, 4, 8])\n",
      "Output shape: torch.Size([1, 4, 8])\n",
      "Attention weights shape: torch.Size([1, 2, 4, 4])\n",
      "Number of heads: 2\n",
      "Multi-Head Attention is working correctly!\n"
     ]
    }
   ],
   "source": [
    "# Multi-Head Attention Implementation\n",
    "# Multiple attention mechanisms running in parallel\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Attention - multiple attention mechanisms in parallel\n",
    "    \n",
    "    This runs multiple self-attention mechanisms simultaneously,\n",
    "    each focusing on different aspects of the input. The outputs\n",
    "    are then combined to create a richer representation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, num_heads: int):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads  # Dimension per head\n",
    "        \n",
    "        # Linear projections for Q, K, V for all heads combined\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # Final linear projection to combine all heads\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass of multi-head attention\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, seq_len, d_model)\n",
    "            mask: Optional mask to prevent attention to certain positions\n",
    "            \n",
    "        Returns:\n",
    "            Output tensor of same shape as input\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, d_model = x.shape\n",
    "        \n",
    "        # Step 1: Create Q, K, V for all heads at once\n",
    "        Q = self.W_q(x)  # (batch_size, seq_len, d_model)\n",
    "        K = self.W_k(x)  # (batch_size, seq_len, d_model)\n",
    "        V = self.W_v(x)  # (batch_size, seq_len, d_model)\n",
    "        \n",
    "        # Step 2: Reshape to separate heads\n",
    "        # From (batch_size, seq_len, d_model) to (batch_size, num_heads, seq_len, d_k)\n",
    "        Q = Q.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = K.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = V.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # Step 3: Calculate attention for each head\n",
    "        # Shape: (batch_size, num_heads, seq_len, seq_len)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        # Apply mask if provided (useful for preventing future information leakage)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        # Apply softmax to get attention weights\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        # Shape: (batch_size, num_heads, seq_len, d_k)\n",
    "        attended_values = torch.matmul(attention_weights, V)\n",
    "        \n",
    "        # Step 4: Concatenate heads back together\n",
    "        # Reshape from (batch_size, num_heads, seq_len, d_k) to (batch_size, seq_len, d_model)\n",
    "        attended_values = attended_values.transpose(1, 2).contiguous().view(\n",
    "            batch_size, seq_len, d_model\n",
    "        )\n",
    "        \n",
    "        # Step 5: Final linear projection\n",
    "        output = self.W_o(attended_values)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "# Test Multi-Head Attention\n",
    "print(\"Testing Multi-Head Attention...\")\n",
    "\n",
    "# Create parameters\n",
    "d_model = 8\n",
    "num_heads = 2  # Use 2 heads for simplicity\n",
    "batch_size = 1\n",
    "seq_len = 4\n",
    "\n",
    "# Create input\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "# Create multi-head attention\n",
    "multi_head_attn = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "# Run it\n",
    "output, attention_weights = multi_head_attn(x)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {attention_weights.shape}\")\n",
    "print(f\"Number of heads: {num_heads}\")\n",
    "\n",
    "print(\"Multi-Head Attention is working correctly!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ea5beb",
   "metadata": {},
   "source": [
    "## Building the Complete Transformer Architecture\n",
    "\n",
    "A complete Transformer consists of several key components:\n",
    "\n",
    "1. **Multi-Head Attention** (We just implemented this)\n",
    "2. **Feed-Forward Networks** - Process the attended information\n",
    "3. **Layer Normalization** - Stabilizes training\n",
    "4. **Residual Connections** - Skip connections that help with gradient flow\n",
    "5. **Positional Encoding** - Provides sequence order information\n",
    "\n",
    "Let's implement each component and then assemble them into a complete Transformer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed-Forward Network Implementation\n",
    "# Processes the attended information through two linear layers\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    Feed-Forward Network - processes information after attention\n",
    "    \n",
    "    This consists of two linear layers with a ReLU activation between them.\n",
    "    The first layer expands the dimension (typically 4x), and the second\n",
    "    contracts it back to the original dimension.\n",
    "    \n",
    "    Architecture:\n",
    "    - First layer: d_model -> 4 * d_model (expansion)\n",
    "    - Activation: ReLU\n",
    "    - Second layer: 4 * d_model -> d_model (contraction)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, d_ff: int = None):\n",
    "        super().__init__()\n",
    "        d_ff = d_ff or 4 * d_model  # Default to 4x expansion\n",
    "        \n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(0.1)  # Small dropout for regularization\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the feed-forward network\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, seq_len, d_model)\n",
    "            \n",
    "        Returns:\n",
    "            Output tensor of same shape as input\n",
    "        \"\"\"\n",
    "        # First linear transformation + ReLU + dropout\n",
    "        x = self.linear1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Second linear transformation\n",
    "        x = self.linear2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Positional Encoding Implementation\n",
    "# Provides sequence position information to the model\n",
    "\n",
    "def get_positional_encoding(seq_len: int, d_model: int):\n",
    "    \"\"\"\n",
    "    Generate positional encodings for the input sequence\n",
    "    \n",
    "    This creates unique position embeddings using sinusoidal functions.\n",
    "    Each position gets a unique \"fingerprint\" that tells the model\n",
    "    where it is in the sequence.\n",
    "    \n",
    "    The encoding uses sine and cosine functions with different frequencies\n",
    "    to create unique patterns for each position.\n",
    "    \"\"\"\n",
    "    pe = torch.zeros(seq_len, d_model)\n",
    "    position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)\n",
    "    \n",
    "    # Create the div_term for the sinusoidal functions\n",
    "    div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n",
    "                        (-math.log(10000.0) / d_model))\n",
    "    \n",
    "    # Apply sin to even indices\n",
    "    pe[:, 0::2] = torch.sin(position * div_term)\n",
    "    # Apply cos to odd indices  \n",
    "    pe[:, 1::2] = torch.cos(position * div_term)\n",
    "    \n",
    "    return pe.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "# Test the components\n",
    "print(\"Testing Feed-Forward Network and Positional Encoding...\")\n",
    "\n",
    "# Test Feed-Forward Network\n",
    "d_model = 8\n",
    "seq_len = 4\n",
    "batch_size = 1\n",
    "\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "ffn = FeedForward(d_model)\n",
    "ffn_output = ffn(x)\n",
    "\n",
    "print(f\"FFN Input shape: {x.shape}\")\n",
    "print(f\"FFN Output shape: {ffn_output.shape}\")\n",
    "\n",
    "# Test Positional Encoding\n",
    "pos_encoding = get_positional_encoding(seq_len, d_model)\n",
    "print(f\"Positional encoding shape: {pos_encoding.shape}\")\n",
    "\n",
    "print(\"Feed-Forward Network and Positional Encoding are working correctly!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete Transformer Implementation\n",
    "# Combining all components into the full architecture\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A single Transformer block - the building block of the Transformer\n",
    "    \n",
    "    This combines:\n",
    "    1. Multi-Head Attention\n",
    "    2. Feed-Forward Network  \n",
    "    3. Layer Normalization (stabilizes training)\n",
    "    4. Residual connections (helps with gradient flow)\n",
    "    \n",
    "    The block processes information in two steps:\n",
    "    Step 1: Multi-Head Attention (determine what's important)\n",
    "    Step 2: Feed-Forward Network (process the information)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, num_heads: int, d_ff: int = None):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Multi-Head Attention\n",
    "        self.attention = MultiHeadAttention(d_model, num_heads)\n",
    "        \n",
    "        # Feed-Forward Network\n",
    "        self.feed_forward = FeedForward(d_model, d_ff)\n",
    "        \n",
    "        # Layer Normalization (stabilizes training)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass through the Transformer block\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, seq_len, d_model)\n",
    "            mask: Optional attention mask\n",
    "            \n",
    "        Returns:\n",
    "            Output tensor of same shape as input\n",
    "        \"\"\"\n",
    "        # Step 1: Multi-Head Attention with residual connection\n",
    "        attn_output, attention_weights = self.attention(x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))  # Residual connection + LayerNorm\n",
    "        \n",
    "        # Step 2: Feed-Forward Network with residual connection  \n",
    "        ffn_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ffn_output))  # Residual connection + LayerNorm\n",
    "        \n",
    "        return x, attention_weights\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    \"\"\"\n",
    "    The Complete Transformer Model\n",
    "    \n",
    "    This implements the full Transformer architecture as described in \n",
    "    \"Attention is All You Need\". It consists of a stack of Transformer blocks\n",
    "    that each process the input in increasingly sophisticated ways.\n",
    "    \n",
    "    Each layer builds upon the previous one:\n",
    "    - Layer 1: Basic word relationships\n",
    "    - Layer 2: Simple sentence structure  \n",
    "    - Layer 3: Complex grammatical patterns\n",
    "    - Layer N: Deep semantic understanding\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size: int, d_model: int, num_heads: int, \n",
    "                 num_layers: int, max_seq_len: int = 512):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "        # Embedding layer - converts token IDs to dense vectors\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        \n",
    "        # Positional encoding (added to embeddings)\n",
    "        self.register_buffer('pos_encoding', get_positional_encoding(max_seq_len, d_model))\n",
    "        \n",
    "        # Stack of Transformer blocks\n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            TransformerBlock(d_model, num_heads) \n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Final layer normalization\n",
    "        self.final_norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass through the entire Transformer\n",
    "        \n",
    "        Args:\n",
    "            x: Input token IDs of shape (batch_size, seq_len)\n",
    "            mask: Optional attention mask\n",
    "            \n",
    "        Returns:\n",
    "            Output embeddings of shape (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = x.shape\n",
    "        \n",
    "        # Step 1: Convert token IDs to embeddings\n",
    "        x = self.embedding(x) * math.sqrt(self.d_model)  # Scale embeddings\n",
    "        \n",
    "        # Step 2: Add positional encoding\n",
    "        x = x + self.pos_encoding[:, :seq_len, :]\n",
    "        \n",
    "        # Step 3: Pass through each Transformer block\n",
    "        attention_weights_list = []\n",
    "        for transformer_block in self.transformer_blocks:\n",
    "            x, attention_weights = transformer_block(x, mask)\n",
    "            attention_weights_list.append(attention_weights)\n",
    "        \n",
    "        # Step 4: Final layer normalization\n",
    "        x = self.final_norm(x)\n",
    "        \n",
    "        return x, attention_weights_list\n",
    "\n",
    "# Test the complete Transformer\n",
    "print(\"Testing the Complete Transformer...\")\n",
    "\n",
    "# Model parameters\n",
    "vocab_size = 1000  # Size of vocabulary\n",
    "d_model = 64      # Model dimension\n",
    "num_heads = 8     # Number of attention heads\n",
    "num_layers = 6    # Number of Transformer blocks\n",
    "max_seq_len = 128 # Maximum sequence length\n",
    "\n",
    "# Create the model\n",
    "transformer = Transformer(vocab_size, d_model, num_heads, num_layers, max_seq_len)\n",
    "\n",
    "# Create dummy input (token IDs)\n",
    "batch_size = 2\n",
    "seq_len = 10\n",
    "input_ids = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "\n",
    "print(f\"Input shape: {input_ids.shape}\")\n",
    "print(f\"Model parameters: {sum(p.numel() for p in transformer.parameters()):,}\")\n",
    "\n",
    "# Run the model\n",
    "output, attention_weights = transformer(input_ids)\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Number of attention weight matrices: {len(attention_weights)}\")\n",
    "print(f\"Each attention matrix shape: {attention_weights[0].shape}\")\n",
    "\n",
    "print(\"SUCCESS! Complete Transformer built and tested!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d546b79c",
   "metadata": {},
   "source": [
    "## Visualizing Attention Patterns\n",
    "\n",
    "Let's create visualizations to see what our Transformer is actually paying attention to. This helps us understand how the model processes information and which relationships it finds important.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention Visualization Functions\n",
    "# Create heatmaps to visualize attention patterns\n",
    "\n",
    "def visualize_attention(attention_weights, tokens=None, layer_idx=0, head_idx=0):\n",
    "    \"\"\"\n",
    "    Visualize attention weights as a heatmap\n",
    "    \n",
    "    Args:\n",
    "        attention_weights: List of attention weight matrices from all layers\n",
    "        tokens: Optional list of token strings for x/y axis labels\n",
    "        layer_idx: Which layer to visualize\n",
    "        head_idx: Which attention head to visualize\n",
    "    \"\"\"\n",
    "    # Get attention weights for the specified layer and head\n",
    "    attn = attention_weights[layer_idx][0, head_idx].detach().numpy()  # Remove batch dimension\n",
    "    \n",
    "    # Create the heatmap\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(attn, cmap='Blues', aspect='auto')\n",
    "    plt.colorbar(label='Attention Weight')\n",
    "    \n",
    "    # Add labels if tokens are provided\n",
    "    if tokens:\n",
    "        plt.xticks(range(len(tokens)), tokens, rotation=45)\n",
    "        plt.yticks(range(len(tokens)), tokens)\n",
    "    \n",
    "    plt.xlabel('Key Position (What we attend TO)')\n",
    "    plt.ylabel('Query Position (What we attend FROM)')\n",
    "    plt.title(f'Attention Heatmap - Layer {layer_idx}, Head {head_idx}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def create_simple_example():\n",
    "    \"\"\"\n",
    "    Create a simple example to demonstrate attention patterns\n",
    "    \"\"\"\n",
    "    print(\"Creating a simple example...\")\n",
    "    \n",
    "    # Create a small model for demonstration\n",
    "    vocab_size = 50\n",
    "    d_model = 32\n",
    "    num_heads = 4\n",
    "    num_layers = 2\n",
    "    \n",
    "    model = Transformer(vocab_size, d_model, num_heads, num_layers)\n",
    "    \n",
    "    # Create a simple sequence (pretend these are meaningful tokens)\n",
    "    # Example: \"The cat sat on the mat\"\n",
    "    sequence = torch.tensor([[1, 5, 8, 12, 1, 15]])  # Token IDs\n",
    "    tokens = [\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n",
    "    \n",
    "    print(f\"Input sequence: {tokens}\")\n",
    "    print(f\"Token IDs: {sequence}\")\n",
    "    \n",
    "    # Run the model\n",
    "    with torch.no_grad():\n",
    "        output, attention_weights = model(sequence)\n",
    "    \n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    print(f\"Number of layers: {len(attention_weights)}\")\n",
    "    print(f\"Number of heads per layer: {attention_weights[0].shape[1]}\")\n",
    "    \n",
    "    return model, attention_weights, tokens\n",
    "\n",
    "# Create and visualize our example\n",
    "model, attention_weights, tokens = create_simple_example()\n",
    "\n",
    "print(\"\\nVisualizing attention patterns...\")\n",
    "print(\"Each cell shows how much attention one word pays to another word.\")\n",
    "print(\"Darker blue = more attention\")\n",
    "\n",
    "# Visualize attention from the first layer, first head\n",
    "visualize_attention(attention_weights, tokens, layer_idx=0, head_idx=0)\n",
    "\n",
    "print(\"\\nLooking at different heads in the same layer...\")\n",
    "# Visualize different heads\n",
    "for head_idx in range(min(4, attention_weights[0].shape[1])):\n",
    "    print(f\"\\nHead {head_idx}:\")\n",
    "    visualize_attention(attention_weights, tokens, layer_idx=0, head_idx=head_idx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd72a854",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "Congratulations! You've successfully built a complete Transformer from scratch. Here's what you've accomplished:\n",
    "\n",
    "### Core Concepts You Now Understand:\n",
    "\n",
    "1. **Self-Attention**: How words can attend to each other and determine relationships\n",
    "2. **Multi-Head Attention**: Why multiple perspectives provide richer understanding\n",
    "3. **Feed-Forward Networks**: The component that processes attended information\n",
    "4. **Positional Encoding**: How the model understands word order\n",
    "5. **Layer Normalization**: Stabilizes training in deep networks\n",
    "6. **Residual Connections**: Skip connections that help with gradient flow\n",
    "\n",
    "### Why This Matters:\n",
    "\n",
    "- **ChatGPT, GPT-4, Claude**: All built on Transformer architecture\n",
    "- **Google Translate**: Uses Transformers for better translations\n",
    "- **Code Completion**: GitHub Copilot, Cursor, etc. use Transformers\n",
    "- **Image Generation**: DALL-E, Midjourney use Transformer-like architectures\n",
    "- **Scientific Research**: Protein folding, drug discovery, etc.\n",
    "\n",
    "### What You Can Do Next:\n",
    "\n",
    "1. **Train Your Own Model**: Use this code as a starting point\n",
    "2. **Experiment with Parameters**: Try different numbers of heads, layers\n",
    "3. **Add Tasks**: Implement language modeling, classification, etc.\n",
    "4. **Study Pre-trained Models**: Look at BERT, GPT, T5 architectures\n",
    "5. **Build Applications**: Create chatbots, translators, code assistants\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d95ba62",
   "metadata": {},
   "source": [
    "## Practical Tips & Common Gotchas\n",
    "\n",
    "### Pro Tips for Working with Transformers:\n",
    "\n",
    "1. **Start Small**: Begin with small models (2-4 layers, 4-8 heads) before scaling up\n",
    "2. **Watch Your Memory**: Attention is O(n²) - longer sequences need more memory\n",
    "3. **Use Mixed Precision**: `torch.cuda.amp` can speed up training significantly\n",
    "4. **Gradient Clipping**: Helps prevent exploding gradients in deep networks\n",
    "5. **Learning Rate Scheduling**: Transformers often benefit from warmup + decay\n",
    "\n",
    "### Common Mistakes to Avoid:\n",
    "\n",
    "1. **Forgetting Positional Encoding**: Without it, your model won't understand word order\n",
    "2. **Wrong Attention Mask**: Can cause information leakage in autoregressive models\n",
    "3. **Not Scaling Embeddings**: Multiply by √d_model to prevent vanishing gradients\n",
    "4. **Too Many Layers Too Fast**: Start simple, then add complexity\n",
    "5. **Ignoring Layer Norm Placement**: Pre-norm vs post-norm can make a big difference\n",
    "\n",
    "### Real-World Considerations:\n",
    "\n",
    "- **Computational Cost**: Attention scales quadratically with sequence length\n",
    "- **Memory Usage**: Large models need lots of GPU memory\n",
    "- **Training Time**: Can take days/weeks for large models\n",
    "- **Data Requirements**: Transformers need lots of data to work well\n",
    "- **Hyperparameter Sensitivity**: Small changes can have big effects\n",
    "\n",
    "### Further Reading:\n",
    "\n",
    "- **Original Paper**: \"Attention is All You Need\" (Vaswani et al., 2017)\n",
    "- **The Illustrated Transformer**: Great visual explanations\n",
    "- **Hugging Face Course**: Practical tutorials on using pre-trained models\n",
    "- **Papers With Code**: See latest implementations and benchmarks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f0488d",
   "metadata": {},
   "source": [
    "## Congratulations! You're Now a Transformer Expert!\n",
    "\n",
    "You've successfully built a complete Transformer from scratch! This is a significant achievement and you now understand one of the most important architectures in modern AI.\n",
    "\n",
    "### What You've Accomplished:\n",
    "\n",
    "- **Built Self-Attention** from first principles  \n",
    "- **Implemented Multi-Head Attention** with multiple perspectives  \n",
    "- **Created Feed-Forward Networks** for information processing  \n",
    "- **Added Positional Encoding** for sequence understanding  \n",
    "- **Constructed Complete Transformer** with all components  \n",
    "- **Visualized Attention Patterns** to see what the model learns  \n",
    "- **Learned Practical Tips** for real-world applications  \n",
    "\n",
    "### You're Ready For:\n",
    "\n",
    "- Building your own language models\n",
    "- Understanding how ChatGPT, GPT-4, and Claude work\n",
    "- Contributing to open-source AI projects\n",
    "- Creating AI applications and tools\n",
    "- Pursuing advanced research in NLP/AI\n",
    "\n",
    "### Final Thoughts:\n",
    "\n",
    "This wasn't easy for me the first time, and it might've been difficult for you too! The fact that you've followed this guide and built this out means you understand just a little more (or maybe a lot) on how the tools we use in our every day lives work. That's the difference between someone who can use AI tools and someone who can build them.\n",
    "\n",
    "---\n",
    "\n",
    "*\"Attention is all you need... but practice and coffee helps too :)\"*\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
